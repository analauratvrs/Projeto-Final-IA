{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Projeto Final de IA: Classificador de Dificuldade de Questões do ENEM\n",
        "---\n",
        "**Aluna:** Ana Laura Tavares Costa\n",
        "\n",
        "**Disciplina:** INF 420 - Inteligência Artificial I\n",
        "\n",
        "**Professor:** Julio Cesar Soares dos Reis\n",
        "\n",
        "**Objetivo do Projeto:** Comparar três modelos de Machine Learning para classificar questões de Matemática do ENEM por nível de dificuldade (Fácil, Médio, Difícil), com base nos microdados públicos do INEP.\n"
      ],
      "metadata": {
        "id": "GbmoPy-V0b2e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Carregamento dos Dados**\n",
        "\n",
        "---\n",
        "\n",
        "**Atenção:** Para executar esta etapa, é necessário que os arquivos de dados do ENEM de 2022 e 2023 já tenham sido baixados e estejam no seu Google Drive.\n",
        "\n",
        "Este notebook espera que os seguintes arquivos estejam localizados na **pasta raiz do seu Google Drive ('Meu Drive')**:\n",
        "1.  `MICRODADOS_ENEM_2022.csv` (contém as respostas dos participantes)\n",
        "2.  `ITENS_PROVA_2022.csv` (contém as informações sobre cada questão da prova)\n",
        "3.  `MICRODADOS_ENEM_2023.csv` (contém as respostas dos participantes)\n",
        "4.  `ITENS_PROVA_2023.csv` (contém as informações sobre cada questão da prova)\n",
        "\n",
        "Caso ainda não tenha os dados, eles podem ser baixados do portal oficial do INEP:\n",
        "* **Link para Download:** [Microdados do ENEM - INEP](https://www.gov.br/inep/pt-br/acesso-a-informacao/dados-abertos/microdados/enem)\n",
        "\n",
        "O código a seguir irá conectar com seu Google Drive e carregar os dois arquivos em DataFrames separados."
      ],
      "metadata": {
        "id": "a3xIR7dt1Tgl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gc\n",
        "import os\n",
        "\n",
        "# modelos e ferramentas de avaliação\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# conexão com google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "\n",
        "# função que processa os dados de um ano do ENEM para extrair as questões de matemática da aplicação regular com seus parâmetros da TRI\n",
        "# retorna um dataframe limpo para ser utilizado ao longo do projeto\n",
        "def processar_dados_enem_anual(ano, pasta_drive):\n",
        "    print(f\"--- Processando dados do ano: {ano} ---\")\n",
        "\n",
        "    path_itens = os.path.join(pasta_drive, f'ITENS_PROVA_{ano}.csv')\n",
        "    path_microdados = os.path.join(pasta_drive, f'MICRODADOS_ENEM_{ano}.csv')\n",
        "\n",
        "    try:\n",
        "        # carrega os dados do ano específico\n",
        "        df_itens = pd.read_csv(path_itens, sep=';', encoding='latin-1')\n",
        "        df_microdados = pd.read_csv(path_microdados, sep=';', encoding='latin-1', usecols=['CO_PROVA_MT', 'TP_PRESENCA_MT', 'TX_RESPOSTAS_MT', 'TX_GABARITO_MT'])\n",
        "\n",
        "        # identifica os códigos da prova regular\n",
        "        df_microdados_presentes = df_microdados[df_microdados['TP_PRESENCA_MT'] == 1]\n",
        "        contagem_provas = df_microdados_presentes['CO_PROVA_MT'].value_counts()\n",
        "        codigos_provas_regulares = contagem_provas.head(4).index.tolist()\n",
        "        print(codigos_provas_regulares)\n",
        "\n",
        "        # filtra para obter as questões de interesse\n",
        "        df_analise_ano = df_itens[\n",
        "            (df_itens['SG_AREA'] == 'MT') &\n",
        "            (df_itens['IN_ITEM_ABAN'] == 0) &\n",
        "            (df_itens['TX_COR'] == 'AZUL') &\n",
        "            (df_itens['CO_PROVA'].isin(codigos_provas_regulares))\n",
        "        ].copy()\n",
        "\n",
        "        # remove duplicatas para ter uma linha por questão única\n",
        "        df_analise_ano = df_analise_ano.drop_duplicates(subset=['CO_ITEM'])\n",
        "\n",
        "        print(f\"Ano {ano} processado com sucesso. Encontradas {len(df_analise_ano)} questões válidas.\")\n",
        "        return df_analise_ano, df_microdados_presentes\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Arquivos para o ano {ano} não encontrados.\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "mZADjrAr7B2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Identificação das provas regulares\n"
      ],
      "metadata": {
        "id": "FXs376oD7qbL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pasta_drive = '/content/drive/MyDrive/'\n",
        "anos_para_processar = [2022, 2023]\n",
        "lista_dataframes_anuais = []\n",
        "lista_df_microdados = []\n",
        "\n",
        "for ano in anos_para_processar:\n",
        "    df_ano, df_md = processar_dados_enem_anual(ano, pasta_drive)\n",
        "    if df_ano is not None and df_md is not None:\n",
        "        lista_dataframes_anuais.append(df_ano)\n",
        "        lista_df_microdados.append(df_md)\n",
        "\n",
        "# junta os dataframes dos anos em um único\n",
        "if lista_dataframes_anuais:\n",
        "    df_itens_total = pd.concat(lista_dataframes_anuais, ignore_index=True)\n",
        "    df_md_total = pd.concat(lista_df_microdados, ignore_index=True)\n",
        "    print(\"\\n--- Processamento Concluído ---\")\n",
        "    print(f\"Dataset final para modelagem criado com {len(df_itens_total)} questões no total.\")\n",
        "    display(df_itens_total.head())\n",
        "    display(df_md_total.head())\n",
        "else:\n",
        "    print(\"\\nNenhum dado foi processado. Verifique os caminhos dos arquivos.\")"
      ],
      "metadata": {
        "id": "79D43IaO72pQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(df_itens_total))\n",
        "print(len(df_md_total))"
      ],
      "metadata": {
        "id": "2nW2rrqLViWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Engenharia de Atributos"
      ],
      "metadata": {
        "id": "GcGSaUlPAazk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if 'df_itens_total' in locals() and not df_itens_total.empty:\n",
        "    # criação do alvo (y)\n",
        "    rotulos = ['Fácil', 'Média', 'Difícil']\n",
        "    df_itens_total['dificuldade_oficial'] = pd.qcut(df_itens_total['NU_PARAM_B'], q=3, labels=rotulos)\n",
        "    y = df_itens_total['dificuldade_oficial']\n",
        "\n",
        "    # seleção e preparação dos atributos (X)\n",
        "    features_iniciais = df_itens_total[['NU_PARAM_A', 'NU_PARAM_C', 'CO_HABILIDADE']]\n",
        "    X = pd.get_dummies(features_iniciais, columns=['CO_HABILIDADE'], prefix='HAB')\n",
        "\n",
        "    print(\"Dataset final (X e y) preparado para a avaliação dos modelos.\")\n",
        "    display(X)\n",
        "    display(y)\n",
        "    display(df_itens_total)\n",
        "else:\n",
        "    print(\"DataFrame 'df_itens_total' não foi criado.\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "JnYygw_QAfpb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Treinamento e Avaliação do Modelo Baseline"
      ],
      "metadata": {
        "id": "2aJQNQQlBHYP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# verifica se X e y existem antes de prosseguir\n",
        "if 'X' in locals() and 'y' in locals():\n",
        "    # dicionário com os modelos a serem testados\n",
        "    modelos = {\n",
        "        'Regressão Logística': LogisticRegression(max_iter=1000, random_state=42),\n",
        "        'Random Forest': RandomForestClassifier(random_state=42),\n",
        "        'SVM (Support Vector Machine)': SVC(random_state=42)\n",
        "    }\n",
        "\n",
        "    metricas_de_avaliacao = ['accuracy', 'f1_weighted']\n",
        "\n",
        "    print(\"--- Avaliando Modelos com Validação Cruzada (k=5) ---\")\n",
        "\n",
        "    # loop para criar um pipeline e avaliar cada modelo\n",
        "    for nome, modelo in modelos.items():\n",
        "        # garante que a padronização é feita corretamente em cada fold\n",
        "        pipeline = Pipeline([\n",
        "            ('scaler', StandardScaler()),\n",
        "            ('classifier', modelo)\n",
        "        ])\n",
        "\n",
        "        scores = cross_validate(pipeline, X, y, cv=5, scoring=metricas_de_avaliacao)\n",
        "\n",
        "        acc_media = scores['test_accuracy'].mean()\n",
        "        acc_std = scores['test_accuracy'].std()\n",
        "        f1_media = scores['test_f1_weighted'].mean()\n",
        "        f1_std = scores['test_f1_weighted'].std()\n",
        "\n",
        "        print(f\"\\nModelo: {nome}\")\n",
        "        print(f\"Acurácia Média: {acc_media:.4f} (Desvio Padrão: {acc_std:.4f})\")\n",
        "        print(f\"F1-Score Ponderado Médio: {f1_media:.4f} (Desvio Padrão: {f1_std:.4f})\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "else:\n",
        "    print(\"Variáveis X e y não foram definidas. Execute a célula anterior.\")"
      ],
      "metadata": {
        "id": "DvPz6s4_BGlD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Adicionando novos features para melhorar a performance dos modelos**"
      ],
      "metadata": {
        "id": "rVONAnhdCIeT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Preparação\n"
      ],
      "metadata": {
        "id": "i2lrSSynEFtk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Serão selecionados os alunos que efetivamente fizeram as provas que filtramos na etapa de filtragem de provas válidas"
      ],
      "metadata": {
        "id": "An7z-R22AuDJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# verifica se os dataframes de partida existem\n",
        "if 'df_itens_total' in locals() and 'df_md_total' in locals():\n",
        "\n",
        "    # extrair a lista de códigos de prova de interesse\n",
        "    # .unique().tolist() garante que pegamos cada código de prova apenas uma vez\n",
        "    print(df_itens_total['CO_PROVA'])\n",
        "    codigos_provas_relevantes = df_itens_total['CO_PROVA'].unique().tolist()\n",
        "\n",
        "    print(f\"Identificados {len(codigos_provas_relevantes)} códigos de prova correspondentes às questões de interesse.\")\n",
        "    print(codigos_provas_relevantes)\n",
        "\n",
        "    # filtrar os alunos que fizeram essas provas\n",
        "    print(f\"\\nFiltrando o DataFrame de {len(df_md_total)} alunos...\")\n",
        "\n",
        "    df_alunos_relevantes = df_md_total[\n",
        "        df_md_total['CO_PROVA_MT'].isin(codigos_provas_relevantes)\n",
        "    ].copy()\n",
        "\n",
        "    print(f\"Encontrados {len(df_alunos_relevantes)} alunos que fizeram as provas de interesse.\")\n",
        "\n",
        "    # cria a amostra aleatória de 100.000 alunos\n",
        "    tamanho_amostra = 100000\n",
        "\n",
        "    # garante que temos alunos suficientes para a amostragem\n",
        "    if len(df_alunos_relevantes) >= tamanho_amostra:\n",
        "        print(f\"\\nCriando uma amostra aleatória de {tamanho_amostra} alunos...\")\n",
        "        df_amostra_alunos = df_alunos_relevantes.sample(n=tamanho_amostra, random_state=42)\n",
        "\n",
        "        print(\"Amostra final criada com sucesso!\")\n",
        "        display(df_amostra_alunos.head())\n",
        "    else:\n",
        "        print(f\"\\nAVISO: O número de alunos relevantes ({len(df_alunos_relevantes)}) é menor que o tamanho da amostra desejado. Usando todos os alunos encontrados.\")\n",
        "        df_amostra_alunos = df_alunos_relevantes\n",
        "\n",
        "\n",
        "    del df_md_total\n",
        "    del df_alunos_relevantes\n",
        "    import gc\n",
        "    gc.collect()\n",
        "    print(\"\\nMemória liberada.\")\n",
        "\n",
        "else:\n",
        "    print(\"DataFrames 'df_itens_total' ou 'df_md_total' não foram encontrados. Execute as células de carregamento.\")"
      ],
      "metadata": {
        "id": "X8BFEm7aAp1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Processamento"
      ],
      "metadata": {
        "id": "zewBYmoYRTfp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Preparação da Estrutura de Contagem"
      ],
      "metadata": {
        "id": "JcZFGTPtwx3E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# verifica se o dataframe de itens existe\n",
        "if 'df_itens_total' in locals():\n",
        "    # cria o dicionário que armazenará as contagens\n",
        "    contagem_features = {}\n",
        "\n",
        "    # pega a lista de todas as questões únicas que vamos analisar\n",
        "    lista_itens_unicos = df_itens_total['CO_ITEM'].unique()\n",
        "\n",
        "    # inicia o \"placar\" para cada questão com os contadores zerados\n",
        "    for item_code in lista_itens_unicos:\n",
        "        contagem_features[item_code] = {'acertos': 0, 'brancos': 0, 'total_participacoes': 0}\n",
        "\n",
        "    print(f\"Estrutura de contagem criada para {len(contagem_features)} questões.\")\n",
        "    print(\"Pronto para iniciar o processamento.\")\n",
        "else:\n",
        "    print(\"DataFrame 'df_itens_total' não foi criado. Execute as células anteriores.\")"
      ],
      "metadata": {
        "id": "Tif0eyVjKD9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Contagem de Acertos Por Questão e Deixadas em Branco"
      ],
      "metadata": {
        "id": "OWci1UQyw6oQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if 'contagem_features' in locals():\n",
        "    print(\"Iniciando o processamento da amostra de alunos (com gabarito corrigido)...\")\n",
        "    total_alunos = len(df_amostra_alunos)\n",
        "\n",
        "    for i, aluno in enumerate(df_amostra_alunos.itertuples()):\n",
        "        if (i + 1) % 10000 == 0:\n",
        "            print(f\"Processando aluno {i + 1} de {total_alunos}...\")\n",
        "\n",
        "        cod_prova_aluno = aluno.CO_PROVA_MT\n",
        "        respostas_aluno = aluno.TX_RESPOSTAS_MT\n",
        "\n",
        "        itens_da_prova = df_itens_total[df_itens_total['CO_PROVA'] == cod_prova_aluno]\n",
        "        itens_da_prova = itens_da_prova.sort_values(by='CO_POSICAO')\n",
        "\n",
        "        for indice_relativo, item in enumerate(itens_da_prova.itertuples()):\n",
        "            cod_item = item.CO_ITEM\n",
        "\n",
        "            if cod_item in contagem_features:\n",
        "                contagem_features[cod_item]['total_participacoes'] += 1\n",
        "\n",
        "                posicao_na_string = indice_relativo\n",
        "                resposta = respostas_aluno[posicao_na_string]\n",
        "\n",
        "                gabarito = item.TX_GABARITO\n",
        "\n",
        "                if resposta == '.':\n",
        "                    contagem_features[cod_item]['brancos'] += 1\n",
        "                else:\n",
        "                    if resposta == gabarito:\n",
        "                        contagem_features[cod_item]['acertos'] += 1\n",
        "\n",
        "    print(f\"\\nProcessamento concluído!\")\n",
        "\n",
        "    df_features_avancadas = pd.DataFrame.from_dict(contagem_features, orient='index')\n",
        "    respostas_preenchidas = df_features_avancadas['total_participacoes'] - df_features_avancadas['brancos']\n",
        "    df_features_avancadas['taxa_acerto'] = (df_features_avancadas['acertos'] / respostas_preenchidas).fillna(0)\n",
        "    df_features_avancadas['taxa_em_branco'] = (df_features_avancadas['brancos'] / df_features_avancadas['total_participacoes']).fillna(0)\n",
        "    df_features_avancadas = df_features_avancadas[['taxa_acerto', 'taxa_em_branco']]\n",
        "    df_features_avancadas.index.name = 'CO_ITEM'\n",
        "\n",
        "    print(\"\\nDataFrame com as novas features:\")\n",
        "    display(df_features_avancadas.head())\n",
        "else:\n",
        "    print(\"Estruturas de dados necessárias não foram encontradas.\")"
      ],
      "metadata": {
        "id": "FOvOgIiCKKfY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Pós-processamento e Criação das Features Finais"
      ],
      "metadata": {
        "id": "bXURXcuxun9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if 'contagem_features' in locals() and any(contagem_features):\n",
        "    df_features_avancadas = pd.DataFrame.from_dict(contagem_features, orient='index')\n",
        "\n",
        "    # calcula as respostas que não foram deixadas em branco\n",
        "    respostas_preenchidas = df_features_avancadas['total_participacoes'] - df_features_avancadas['brancos']\n",
        "\n",
        "    # calcula a taxa de acerto, tratando a divisão por zero (caso uma questão só tenha tido respostas em branco)\n",
        "    df_features_avancadas['taxa_acerto'] = (df_features_avancadas['acertos'] / respostas_preenchidas).fillna(0)\n",
        "\n",
        "    # calcula a taxa de respostas em branco\n",
        "    df_features_avancadas['taxa_em_branco'] = (df_features_avancadas['brancos'] / df_features_avancadas['total_participacoes']).fillna(0)\n",
        "\n",
        "    # seleciona apenas as colunas finais que nos interessam e define o índice\n",
        "    df_features_avancadas = df_features_avancadas[['taxa_acerto', 'taxa_em_branco']]\n",
        "    df_features_avancadas.index.name = 'CO_ITEM'\n",
        "\n",
        "    print(\"DataFrame com as novas features ('taxa_acerto' e 'taxa_em_branco') foi calculado com sucesso:\")\n",
        "    display(df_features_avancadas.head())\n",
        "else:\n",
        "    print(\"Dicionário 'contagem_features' está vazio ou não existe. O loop pode não ter sido executado corretamente.\")"
      ],
      "metadata": {
        "id": "1S823zVZLhIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Junção dos features e preparação para ML"
      ],
      "metadata": {
        "id": "FNYH62Y2v3u1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if 'df_itens_total' in locals() and 'df_features_avancadas' in locals():\n",
        "    # define o índice para a junção correta\n",
        "    df_base = df_itens_total.set_index('CO_ITEM')\n",
        "    df_novas_features = df_features_avancadas.copy()\n",
        "    df_novas_features.index = pd.to_numeric(df_novas_features.index)\n",
        "\n",
        "    # cria a coluna alvo 'dificuldade_oficial'\n",
        "    rotulos = ['Fácil', 'Média', 'Difícil']\n",
        "    df_base['dificuldade_oficial'] = pd.qcut(df_base['NU_PARAM_B'], q=3, labels=rotulos)\n",
        "\n",
        "    features_base = df_base[['NU_PARAM_A', 'NU_PARAM_C', 'CO_HABILIDADE']]\n",
        "    features_base_encoded = pd.get_dummies(features_base, columns=['CO_HABILIDADE'], prefix='HAB')\n",
        "\n",
        "    # junta as features de base com as avançadas\n",
        "    X_avancado = pd.merge(\n",
        "        left=features_base_encoded,\n",
        "        right=df_novas_features,\n",
        "        left_index=True,\n",
        "        right_index=True,\n",
        "        how='inner'\n",
        "    )\n",
        "\n",
        "    # junta as features finais com o alvo\n",
        "    df_modelagem_final = pd.concat([X_avancado, df_base['dificuldade_oficial']], axis=1).dropna()\n",
        "\n",
        "    print(\"DataFrame final pronto para modelagem!\")\n",
        "    display(df_modelagem_final.head())\n",
        "else:\n",
        "    print(\"Um dos DataFrames de partida não foi encontrado.\")"
      ],
      "metadata": {
        "id": "4XgCl6guimsz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Avaliação Final"
      ],
      "metadata": {
        "id": "WKHgG-CzvFNf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if 'df_modelagem_final' in locals() and not df_modelagem_final.empty:\n",
        "\n",
        "    # separa as features (X) e o alvo (y)\n",
        "    X_final = df_modelagem_final.drop('dificuldade_oficial', axis=1)\n",
        "    y_final = df_modelagem_final['dificuldade_oficial']\n",
        "\n",
        "    # dicionário com os modelos\n",
        "    modelos = {\n",
        "        'Regressão Logística': LogisticRegression(max_iter=1000, random_state=42),\n",
        "        'Random Forest': RandomForestClassifier(random_state=42),\n",
        "        'SVM (Support Vector Machine)': SVC(random_state=42)\n",
        "    }\n",
        "\n",
        "    # define as métricas\n",
        "    metricas_de_avaliacao = ['accuracy', 'f1_weighted']\n",
        "\n",
        "    print(\"--- Avaliando Modelos com o Conjunto de Features Corrigido ---\")\n",
        "\n",
        "    # loop para avaliar cada modelo\n",
        "    for nome, modelo in modelos.items():\n",
        "        pipeline = Pipeline([ ('scaler', StandardScaler()), ('classifier', modelo) ])\n",
        "        scores = cross_validate(pipeline, X_final, y_final, cv=5, scoring=metricas_de_avaliacao)\n",
        "\n",
        "        acc_media = scores['test_accuracy'].mean()\n",
        "        f1_media = scores['test_f1_weighted'].mean()\n",
        "\n",
        "        print(f\"\\nModelo: {nome}\")\n",
        "        print(f\"Acurácia Média: {acc_media:.4f}\")\n",
        "        print(f\"F1-Score Ponderado Médio: {f1_media:.4f}\")\n",
        "        print(\"-\" * 50)\n",
        "else:\n",
        "    print(\"DataFrame 'df_modelagem_final' não foi criado.\")"
      ],
      "metadata": {
        "id": "SkcUhQ-M8t6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ajuste Fino e Comparação Definitiva dos Modelos"
      ],
      "metadata": {
        "id": "5EplPIcGvRFM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "import pandas as pd\n",
        "\n",
        "# verifica se o dataframe de modelagem final existe\n",
        "if 'df_modelagem_final' in locals() and not df_modelagem_final.empty:\n",
        "\n",
        "    # separa as features (X) e o alvo (y)\n",
        "    X_final = df_modelagem_final.drop('dificuldade_oficial', axis=1)\n",
        "    y_final = df_modelagem_final['dificuldade_oficial']\n",
        "\n",
        "    # padroniza os dados, pois é necessário para LR e SVM\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X_final)\n",
        "\n",
        "    # grade para Regressão Logística\n",
        "    param_grid_lr = {\n",
        "        'C': [0.1, 1, 10, 100]\n",
        "    }\n",
        "\n",
        "    # grade para SVM\n",
        "    param_grid_svm = {\n",
        "        'C': [0.1, 1, 10, 100],\n",
        "        'gamma': [1, 0.1, 0.01]\n",
        "    }\n",
        "\n",
        "    # grade para Random Forest\n",
        "    param_grid_rf = {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'max_depth': [5, 10, 20]\n",
        "    }\n",
        "\n",
        "    # lista contendo os modelos e seus parâmetros para iterar\n",
        "    modelos_para_tuning = [\n",
        "        ('Regressão Logística', LogisticRegression(max_iter=2000, random_state=42), param_grid_lr),\n",
        "        ('SVM (Support Vector Machine)', SVC(random_state=42), param_grid_svm),\n",
        "        ('Random Forest', RandomForestClassifier(random_state=42), param_grid_rf)\n",
        "    ]\n",
        "\n",
        "    # lista para guardar os melhores resultados de cada modelo\n",
        "    melhores_resultados = []\n",
        "\n",
        "    print(\"--- Iniciando Ajuste de Hiperparâmetros para Todos os Modelos ---\")\n",
        "\n",
        "    # itera sobre cada modelo e executa o GridSearchCV ---\n",
        "    for nome, modelo, params in modelos_para_tuning:\n",
        "        print(f\"\\n--- Ajustando: {nome} ---\")\n",
        "\n",
        "        # n_jobs=-1 usa todos os processadores para acelerar a busca\n",
        "        grid_search = GridSearchCV(estimator=modelo, param_grid=params, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "        grid_search.fit(X_scaled, y_final) # usa os dados padronizados\n",
        "\n",
        "        print(f\"Melhores parâmetros encontrados: {grid_search.best_params_}\")\n",
        "        print(f\"Melhor acurácia (com validação cruzada): {grid_search.best_score_:.4f}\")\n",
        "\n",
        "        melhores_resultados.append({\n",
        "            'Modelo': nome,\n",
        "            'Melhor Acurácia': grid_search.best_score_,\n",
        "            'Melhores Parâmetros': grid_search.best_params_\n",
        "        })\n",
        "\n",
        "    # exibe a tabela para comparação\n",
        "    print(\"\\n\\n--- Tabela Comparativa Final (Após Ajuste de Hiperparâmetros) ---\")\n",
        "    df_comparacao_final = pd.DataFrame(melhores_resultados)\n",
        "    display(df_comparacao_final)\n",
        "\n",
        "else:\n",
        "    print(\"DataFrame 'df_modelagem_final' não foi criado.\")"
      ],
      "metadata": {
        "id": "q-yIOaLGO-cJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Geração dos gráficos"
      ],
      "metadata": {
        "id": "W-Dv7A4j4fNm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "dados_grafico = {\n",
        "    'Modelo': ['Regressão Logística', 'Random Forest', 'SVM'] * 3,\n",
        "    'Fase': ['1. Baseline'] * 3 + ['2. Features Avançadas'] * 3 + ['3. Otimizado'] * 3,\n",
        "    'Acurácia': [\n",
        "        0.3477, 0.4176, 0.3719,\n",
        "        0.3941, 0.5000, 0.3601,\n",
        "        0.4059, 0.5118, 0.3941\n",
        "    ]\n",
        "}\n",
        "df_grafico = pd.DataFrame(dados_grafico)\n",
        "plt.figure(figsize=(8, 5.2))\n",
        "palette = \"viridis\"\n",
        "ax = sns.barplot(data=df_grafico, x='Modelo', y='Acurácia', hue='Fase', palette=palette)\n",
        "\n",
        "plt.ylabel('Acurácia Média (Validação Cruzada, k=5)', fontsize=14)\n",
        "plt.xlabel('')\n",
        "plt.ylim(0, 0.6)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
        "plt.legend(title='Fase do Projeto', fontsize=12, title_fontsize=13)\n",
        "\n",
        "ax.tick_params(axis='x', labelsize=12)\n",
        "ax.tick_params(axis='y', labelsize=12)\n",
        "\n",
        "for p in ax.patches:\n",
        "    if p.get_height() > 0:\n",
        "        ax.annotate(format(p.get_height(), '.2%'),\n",
        "                    (p.get_x() + p.get_width() / 2., p.get_height()),\n",
        "                    ha='center', va='center',\n",
        "                    xytext=(0, 8),\n",
        "                    textcoords='offset points',\n",
        "                    fontsize=11)\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.savefig('grafico_etapas.pdf', bbox_inches='tight')\n",
        "# plt.savefig('figuras/grafico_comparativo_300dpi.png', dpi=300, bbox_inches='tight')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "cpXuuFiQ4elc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importância de cada feature no Random Forest"
      ],
      "metadata": {
        "id": "1QMJxqTOIyUr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "if 'df_modelagem_final' in locals():\n",
        "    print(\"Analisando a importância de cada feature para o melhor modelo (Random Forest)...\")\n",
        "\n",
        "    X_final = df_modelagem_final.drop('dificuldade_oficial', axis=1)\n",
        "    y_final = df_modelagem_final['dificuldade_oficial']\n",
        "\n",
        "    modelo_final_rf = Pipeline([\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('classifier', RandomForestClassifier(\n",
        "            n_estimators=50,\n",
        "            max_depth=20,\n",
        "            random_state=42\n",
        "        ))\n",
        "    ])\n",
        "\n",
        "    modelo_final_rf.fit(X_final, y_final)\n",
        "\n",
        "    importancias = modelo_final_rf.named_steps['classifier'].feature_importances_\n",
        "    nomes_features = X_final.columns\n",
        "\n",
        "    df_importancia = pd.DataFrame({'feature': nomes_features, 'importance': importancias})\n",
        "    df_importancia = df_importancia.sort_values(by='importance', ascending=False)\n",
        "\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.barplot(x='importance', y='feature', data=df_importancia.head(15), palette='viridis')\n",
        "\n",
        "    plt.title('Top 15 Features Mais Importantes para o Modelo Random Forest', fontsize=16)\n",
        "    plt.xlabel('Importância Relativa', fontsize=12)\n",
        "    plt.ylabel('Feature', fontsize=12)\n",
        "    plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "else:\n",
        "    print(\"DataFrame 'df_modelagem_final' não foi encontrado. Execute as células de preparação e unificação primeiro.\")"
      ],
      "metadata": {
        "id": "7-JVRXjmI2a7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}